---
title: "Replication of ___How Quick Decisions Illuminate Moral Character___ by Critcher et al (2013, Social Psychological and Personality Science)"
subtitle: "Original Authors: Clayton R. Critcher, Yoel Inbar, & David A. Pizarro"
author: "Harley Clifton, Sara Hamidi, Prosperity Land, & Bella Mullen"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

The original study examined how the speed of decision-making influences perception of moral character. It found that decision speed helps classify whether a choice is moral or immoral depending on the perceived certainty of the decision. Quick decisions perceived as moral led to more positive character evaluations, while quick immoral decisions resulted in harsher judgments. In contrast, slower decisions were viewed as less certain, leading to more moderate evaluations.

Replicating Experiment 1 from “How Quick Decisions Illuminate Moral Character” offers an opportunity to explore how decision speed affects character judgments. In this experiment, participants will read a scenario about two characters who find separate cash-filled wallets in a grocery store parking lot. One character decides quickly, while the other takes longer. Participants will be randomly assigned to either a moral or immoral condition. Those in the moral condition will learn that both characters returned the wallets, while those in the immoral condition will read that both characters kept the money. Participants will then rate each character based on perceived decision speed, moral principles, decision certainty, and impulsiveness. A two-way ANOVA will then be used to analyze whether decision speed consistently affects moral judgments. 

$~$

## Design Overview

### Manipulated Variables

1.  Decision speed (quick or slow): The researchers manipulated the speed by informing participants whether Justin or Nate made their decisions quickly or after "long and careful deliberation".

2.  Moral conditions (immoral or moral act): The moral decision was assigned between subjects, meaning participants were only exposed to one moral condition. Although this assignment was random, it counts as a manipulation because the researchers intentionally varied the condition to study differences in participant perception.

###### 
### Measurements

-   Four primary measures were taken in Experiment 1
    -   Moral character evaluation
    -   Quickness
    -   Certainty
    -   Emotional impulsivity

###### 
### Study Design

Experiment 1 is a mixed design because the key condition was assigned between-participants, but multiple measures were recorded for each subject, making it also within-participants. Each participant was exposed to only one type of moral outcome, either the characters acted morally or immorally.

#### Repetition in Measures

In experiment 1, four measures were repeated for quick and slow decision speeds. Each participant provided responses to the same set of questions (e.g., moral character evaluation, certainty, emotional impulsivity) for each condition.

##### Consequences of Design Alterations

Modifying the study to a within-participant design, where participants experience both moral and immoral conditions, may elicit biased responses. A within-participant design may negatively influence participant judgments by making it harder to evaluate behaviors in each condition independently. It may also create demand characteristics, where participants guess the study's purpose and modify their responses as a result.

#### Reducing Demand characteristics

Preserving the mixed design and randomly assigning conditions minimizes demand characteristics. This approach reduces the influence of prior knowledge on evaluations and prevents participants from making direct comparisons between conditions. Random assignment also minimizes bias and percents participants from recognizing patterns that could reveal the study's intent.

$~$

## Methods

### Power Analysis

The original study did not provide sufficient statistical information to conduct a power analysis. Specifically, it did not report the mean difference between decision speeds or the corresponding standard error, nor did it provide the means and standard errors for the interaction between decision and speed. Additionally, the figure for Experiment 1 lacked error bars, making it impossible to visually estimate variability or calculate effect sizes. Without these metrics it is difficult to perform a reliable power analysis to determine the required sample size for replication. Therefore, we are using an alternative approach by applying the standard procedure of multiplying the original sample size (n = 119) by 2.5, resulting in a desired sample size of N = 289 participants.

#### Planned Sample

Our planned sample size is 289 participants.

###### 
### Materials

To replicate this experiment, we will use JSPsych to create an online study involving written scenarios and participant surveys. Participants will read scenarios involving Justin and Nate, and then rate them on quickness, moral character, decision certainty, and impulsivity. The original author of the study has kindly provided us with a detailed document outlining the script and protocol used for Experiment 1, which we will use to ensure that our replication remains as close as possible to the original study.

**Experimental paradigm link: <https://ucsd-psych201a.github.io/critcher2013_1/>**

###### 
### Procedure

We plan to follow the procedure outlined by the authors.

Experiment 1:

-   "Participants read about both Justin and Nate, two men who each independently came upon two separate cash-filled wallets in the parking lot of a local grocery store. Justin 'was able to decide quickly' what to do. Nate 'was only able to decide after long and careful deliberation.'"

-   "Participants assigned to the moral condition learned both men 'did not steal the money but instead left the wallet with customer service.' Those in the immoral condition learned instead that both men 'pocketed the money and drove off.'"

-   "Participants were asked to rate the quickness, moral character evaluation, decision certainty, and emotional impulsivity of the agents on 1-7 scales."

-   Quickness: "participants indicated how quickly (vs. slowly) the decision was made"

-   Moral Character Evaluation: "participants assess the agents’ underlying moral principles and standards...by asking whether the agent: “has entirely good (vs. entirely bad) moral principles,” “has good (vs. bad) moral standards,” and “deep down has the moral principles and knowledge to do the right thing.”

-   Certainty: "Participants indicated 'how conflicted \[each\] felt when making his decision', 'how many reservations \[each\] had', whether the target 'was quite certain in his decision', and 'how far \[each\] was from choosing the alternate course of action.'"

-   Emotional Impulsivity: "Participants indicated to what extent the person remained “calm and emotionally contained” (reverse-scored) and 'became upset and acted without thinking.'"

###### 
### Analysis Plan

Our primary focus in this study is to evaluate moral character based on decision morality and decision speed; specifically, the determine whether there is an interaction between morality condition and decision speed on moral character judgement.

Once our data is collected, we will exclude responses with non-answers and data from participants who fail our attention check at the end of the survey, which we will implement to assess attentiveness during the experiment. As a sanity check, participants should always rate Justin _(who is able to decide quickly)_ as faster (higher numerical value) than Nate _(who only make his decision after careful consideration)_. If participants fail to do so, even if they rate them at the same speed, we have reason to question whether they were adequately reading the prompts and thus their data will be excluded from the analysis. Any participants who respond the same way to all numerical response questions also give us reason to question the validity of their responses, and their data will be excluded from the analysis as well.

With these measures, we hope to ensure that the data is good quality and ready for analysis.

For the statistical analysis, our main question of interest involves testing the effect of two factors (speed and decision) and their interaction on the response variable (moral character judgement); thus a type III, two-way analysis of variance (ANOVA) test is appropriate. A type III ANOVA tests each main effect and interaction, adjusting for all other factors and interactions in the model.

The results of the type III two-way ANOVA will tell us whether a model with the interaction term is better than just a model with all the predictors additively. After the best model is found, the ANOVA assumptions and model diagnostics will be assessed and the results will be interpretation for ease of understanding.

###### 
#### Differences from Original Study

In our replication project, we will introduce several features that will differ from the original experiment. One difference is the addition of an attention check at the end of the experiment where we will ask participants to name one of the characters from the scenarios. This attention check is designed to ensure participants are engaged throughout the study and will be used to assist in the data cleaning process. Another difference is that our participants will not be limited to undergraduate students since we are conducting an online study. Additionally, our sample size will differ from the original study because we could not conduct a power analysis based on the information provided in the article. Instead, we opted for a rule-of-thumb multiplier to determine our required sample size. These differences may affect the replication by introducing variability not present in the original study.

###### 
### Methods Addendum (Post Data Collection)

In our class, we are one of three separate groups running replications of this study. Due to budget constraints, there were not enough funds for all three groups to run 289 participants. Therefore, each group was allowed to run 100 participants for their replication study. Data was analysed from our individual group's results, as well as the combined data from all 3 studies (only from participants that passed the groups' unique attention checks).

###### 
#### Actual Sample

**Data Exclusion Criteria:**
Our exclusion criteria are as follows

-   submissions with any non-answers
-   participants that fail the attention check
-   participants who incorrectly rate Justin as *slower* (lower numerical value) than Nate
-   participants who respond the same to all numerical response questions

**Official Sample Size:**
After these the data exclusions and filtering were applied to the final combined data from all three groups, the resulting sample sizes was **220**.

###### 

#### Differences from pre-data collection methods plan

After collecting our group's portion of the data, we quickly realized there were only 16 participants (out of our sample of 100) who responded to every rating question—so the initial intention to only use data from those participants had to be changed in the hopes of increased power. Therefore, we decided that instead, we would only use data from participants that had at least one non-NA response in each unique combination of Quickness, Character Judgments, and Decision Speed conditions. This would make it reasonable to still conduct our main analyses of interest on these data, without risking muddled results due to abundant missing comparisons. 

$~$

## Results

What follows is the analysis and results of the three groups combined data. See the Appendix for more detailed analysis of our group's individual data analysis and results.

### Data preparation

For our preparation plan, we removed observations that met our exclusion criteria. We also pivoted our dataset to a long format to expedite data visualization Lastly, we planned to investigate any suspicious outliers to determine whether they were problematic before removing them.

Data preparation following the analysis plan.

```{r include=F}
### Data Preparation

#### Load Relevant Libraries and Functions
library(tidyverse)
library(car)
library(patchwork)
library(lme4)
```

```{r}
#### Import data
cdf <- read_csv("../data/quick_decisions_all_clean_data.csv")
colfactor <- c("condition", "ID", "FastorSlow", "measure")
cdf[colfactor] <- lapply(cdf[colfactor], as.factor)
```

As previously mentioned, our initial plan was altered so data was only used from participants who had at least one non-NA response for each unique combination of character judgement and quickness conditions for both the fast and slow categories.

```{r}
uniqueIDs <- cdf %>%
  # Focus only on the relevant combinations of measure and FastorSlow
  filter(measure %in% c("quickness", "character") & FastorSlow %in% c("f", "s")) %>%
  group_by(ID) %>%
  # Check if each ID has at least one non-NA response for all combinations
  filter(all(!is.na(value[measure == "quickness" & FastorSlow == "f"])) &
         all(!is.na(value[measure == "quickness" & FastorSlow == "s"])) &
         all(!is.na(value[measure == "character" & FastorSlow == "f"])) &
         all(!is.na(value[measure == "character" & FastorSlow == "s"]))) %>%
  ungroup() %>%
  distinct(ID)

# Filter the original dataset to keep only rows with those IDs
filteredData <- cdf %>% filter(ID %in% uniqueIDs$ID)

distinctIDcount <- filteredData %>% summarise(count = n_distinct(ID))
uninum <- distinctIDcount[[1]]
```

From the combined data, there are `r uninum` unique participants that have at least one non-NA response in each unique combination of Quickness, Character Judgments, and Decision Speed conditions.

```{r}
#### Data exclusion / filtering

excluData <- filteredData %>%
  group_by(ID) %>%   # Group by ID
  
  # all of these responses are ones from the groups that 
  # passed their individual attention checks
  
  # removed any ids that rate the fast person as slower than the slow person
  filter(!any(measure == "quickness" &
              FastorSlow == "f" &
              value < value[measure == "quickness" & FastorSlow == "s"])) %>%
  
  # filter out ids that have all the same non-na response values
  filter(n_distinct(value, na.rm = TRUE) > 1) %>%
  
  ungroup() # Ungroup after filtering
```

Filtering out responses from any participants that incorrectly rated the slow condition as faster or equal to the quick condition left us with a sample of 223 unique participant IDs. 
Then excluding responses from any participants whose non-NA responses were all the same left us with a final combined sample size of 220.

Next, each participants' average rating for character and quickness, split by the fast and slow categories, were calculated for visualization purposes and statistical analyses.

```{r}
#### Prepare data for analysis - create columns etc.

filtereDData <- excluData %>%
  filter(measure %in% c("quickness", "character"))


# Calculate averages for "character" measure grouped by ID and FastorSlow
characterAverages <- filtereDData %>%
  filter(measure == "character") %>%
  group_by(ID, FastorSlow, condition) %>%
  summarize(
    response_reverse = mean(as.numeric(response_reverse), na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(measure = "character") # Ensure the measure column remains "character"


# Keep only the "quickness" rows
quicknessData <- filtereDData %>%
  filter(measure == "quickness") %>%
  select(ID, FastorSlow, condition, response_reverse, measure)


# Combine the averaged "character" data with the "quickness" data
tidyDF <- rbind(quicknessData, characterAverages)
```

```{r}
## Descriptive Statistics

# Determine whether we had a balanced design

## sample size of each randomized condition group
tidyDF %>%
  group_by(condition) %>%
  summarize(subject_count = n_distinct(ID), .groups = "drop")
```

After all of the data cleaning and data exclusions, the combined data ended up with a perfectly balanced design. 110 participants were randomly assigned to the immoral condition, and 110 were randomly assigned to the moral condition.

###### 
### Confirmatory analysis

The combined data were used to create a bar graph and line plot to explore the potential for an interaction between decision speed and morality condition on moral character judgement. 

```{r}
### Data Visualization (Combined Data)

# Filter data to include only rows where measure is "character"
characterData <- tidyDF %>%
  filter(measure == "character") %>%
  mutate(response_reverse = as.numeric(response_reverse), # response is numeric
         FastorSlow = as.factor(FastorSlow), # Convert FastorSlow to factor
         condition = as.factor(condition), # Convert condition to factor
         ID = as.factor(ID)) # Convert ID to factor


# Create the bar plot
barPlot <- ggplot(characterData, 
       aes(x = condition, y = response_reverse, fill = FastorSlow)) +
  stat_summary(fun = mean, # Compute mean of response
               geom = "bar", # Use bars to represent the summary
               position = position_dodge(width = 0.8), width = 0.8) +
  stat_summary(fun.data = mean_se, # Add error bars (mean ± standard error)
               geom = "errorbar",
               position = position_dodge(width = 0.8), width = 0.2) +
  labs(x = "Morality Condition",
       y = "(Positive) Moral Character Evaluation",
       fill = "Decision Speed") +
  scale_fill_manual(values = c("f" = "orange1", "s" = "magenta3"),
                    labels = c("f" = "Quick", "s" = "Slow")) +
  scale_y_continuous(breaks = 1:7) +
  coord_cartesian(ylim = c(1, 7)) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.position = "none")


# Create a line plot
linePlot <- ggplot(characterData, 
       aes(x = condition, y = response_reverse, 
           color = FastorSlow, group = FastorSlow)) +
  stat_summary(fun = mean, # Compute mean of response
               geom = "line", # Use lines to connect the means
               linewidth = 1) +
  stat_summary(fun = mean, # Add points for the means
               geom = "point",  size = 3) +
  stat_summary(fun.data = mean_se, # Add error bars (mean ± standard error)
               geom = "errorbar", width = 0.4) +
  labs(x = "Morality Condition",
       y = "(Positive) Moral Character Evaluation",
       color = "Decision Speed") +
  scale_color_manual(values = c("f" = "orange1", "s" = "magenta3"), 
                     labels = c("f" = "Quick", "s" = "Slow")) +
  scale_y_continuous(breaks = 1:7) +
  coord_cartesian(ylim = c(1, 7)) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.position = "right",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10))


# Combine plots using patchwork
combinedPlot <- (barPlot + linePlot) +
  plot_layout(widths = c(2, 1)) + # Bar plot takes up 2/3, line plot 1/3
  plot_annotation(
    title = "Character Judgements by Morality Condition and Decision Speed  \n (Combined Data)",
    theme = theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold")))


# Display the combined plot
print(combinedPlot)
```

The intersecting lines in the line plot indicate a very high likelihood of an interaction prior to any statistical tests being ran. 

Following this evidence, a our model was fit; a mixed effect model was chosen to appropriately account for the repeated measures on participants with a random intercept. Then, a type III, two-way ANOVA will be run on the model to assess if the interaction is needed in the model to explain the patterns in the data (aka, if it is significant). A type III ANOVA is appropriate to test models with interaction terms, and a two-way ANOVA is needed because we have two 2-level factors being tested: (1) Decision Speed (fast or slow), and (2) Morality Condition (moral or immoral).

```{r}
# Fit a mixed model (accounts for ID) with the interaction of interest
intmodc <- lmer(response_reverse ~ condition * FastorSlow + (1 | ID), 
               data = characterData)

# Use a Type III, 2x2 Two-way ANOVA test
# the results will tell us if the interaction term in needed in the model
Anova(intmodc, type = "III")
```

The results of the ANOVA are consistent with what we saw in our figure; the interaction between decision speed and morality condition has a significant effect on moral character judgement. In short, the experiment replicated!



$~$


### Exploratory analyses

To assess how well this model fits and explains the data, diagnostics plots were used to asses our model assumptions.


#### Model Diagnostics

**Independence Assumption:**
Assessing *Independence of Observations* is a thought exercise. No one participant's responses should impact another participant's responses, therefore there is no reason to suspect violations of this assumption.


To assess *Linearity* and *Homogeneity of Variance*, the Residuals vs Fitted plot will be investigated.

```{r}
fitted_vals <- fitted(intmodc)
residuals <- resid(intmodc)

plot(fitted_vals, residuals,
    pch = 19, col = "black",
    main = "Residuals vs Fitted",
    xlab = "Fitted Values",
    ylab = "Residuals")
abline(h = 0, col = "black")  # Horizontal line at 0
lines(lowess(fitted_vals, residuals), col = "red", lwd = 2)
```

**Linearity Assumption:**
The red line does a fairly good job of following the horizontal zero line, indicating little evidence of any missed curvature that our model would fail to account for. Therefore, there is not enough deviation to suspect problematic violation of this assumption.

**Homoskedacity:**
To assess homogeneity of variance, we are looking for drastic changes in vertical spread of point on the residual vs. fitted plot. For fitted values between 2 and 4.5 there is an increasing fanning pattern, and fitted values between 4.5 and 7 illustrated a decreasing fanning pattern. This double fanning, or diamond shape, indicates a clear violation of the homoskedacity assumption.

**Normality of Residuals:**
To assess *Normality of Residuals*, the Normal Q-Q Plot and a histogram of residuals will be referenced.
```{r}
# Normal Q-Q Plot
qqplot <- ggplot(data.frame(resid = resid(intmodc)), aes(sample = resid)) +
  stat_qq(size = 1.5, shape = 1) +  # Adjust point size and shape
  stat_qq_line() +  # Add the Q-Q line
  labs(title = "Normal Q-Q Plot",
       x = "Theoretical Quantiles",
       y = "Sample Quantities") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title = element_text(size = 11), 
        axis.text = element_text(size = 11), 
        aspect.ratio = 1 / 2)  # Makes the plot rectangular (width > height)


# Histogram of Residuals
eij = residuals(intmodc)
normhist <- ggplot(data.frame(resid = eij), aes(x = resid)) +
  geom_histogram(binwidth = 0.5, color = "black", fill = "lightgray") +
  labs(title = "Histogram of Residuals", 
       x = "Residuals", 
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title = element_text(size = 11),
        axis.text = element_text(size = 11),
        aspect.ratio = 2 / 3)  # Makes the plot rectangular (width > height)


# Combine plots using patchwork
normresid_plot <- qqplot + normhist +
  plot_layout(ncol = 2)


# Display the combined plot
print(normresid_plot)
```

The Normal Q-Q plot and Histogram of Residuals both show evidence of a heavy-tailed pattern, which indicates potential for problematic violation of the normality assumption.


**Multicollinearity:**
The potential for *Multicollinearity* was explored by investigating the variance inflation factors (vifs).
```{r}
car::vif(intmodc)
```
```{r}
sqrt(2.478771)
```
The square root of the interaction's variance inflation factor tells us that the standard error for the interaction term is 1.574 times larger due to multicollinearity (the sharing of information) with other variables in the model than it would have been otherwise. 



###### 
#### Interpretation of Results

As previously mentioned, the study replicated! Our model and statistical analysis also indicated that the interaction term is necessary in the model. 

Next, a confidence interval for the interaction term was created to give more insight into what this meaningfully tells us.
```{r}
summary(intmodc)
confint(intmodc)
```

**Interpreting the Interaction:**
Making a moral decision slowly results in _lower_ moral character judgement, compared to moral decisions made quickly. The opposite is true for immoral decisions; making an immoral decision slowly results in _higher_ moral character judgement compared to immoral decisions made quickly. In more general terms, the effect of decision speed on moral character judgement is very dependent on the morality of the decision made.


$~$

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.

The results from our confirmatory analysis indicate that the interaction between decision speed and decision morality significantly influences character evaluations. Like the original study, we found that quick moral decisions led to more positive character evaluations, while slow immoral decisions resulted in harsher character evaluations. Our ANOVA corroborates the statistical significance of the interaction between these variables, therefore confirming that we successfully replicated the key outcome of the original study.

###### 
### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt. None of these need to be long.

(b) Although there are a few minor differences between our study and the original, such as collecting data online and using different exclusion criteria, the statistical significance of the interaction and the replication of the original findings suggest that these modifications did not affect the outcome.

(c) There were no objections raised by the original authors regarding our replication attempt. We made every effort to replicate their procedure as closely as possible, including using the exact same written scenarios they composed for Experiment 1.

$~$

## Credit Taxonomy

**Harley Clifton**: Conceptualization, Formal Analysis, Investigation, Data Curation, Writing, Visualization, Project Administration. **Sara Hamidi**: Conceptualization, Software, Investigation, Resources, Data Curation. **Prosperity Land**: Conceptualization, Investigation, Writing, Visualization. **Isabella Mullen**: Conceptualization, Software, Investigation, Resources, Data Curation, Project Administration.


$~$

## Appendix

### Our Group's Findings, Analysis, & Results


#### Data preparation

For our preparation plan, we removed observations that met our exclusion criteria. We also pivoted our dataset to a long format to expedite data visualization Lastly, we planned to investigate any suspicious outliers to determine whether they were problematic before removing them.

Data preparation following the analysis plan.

```{r}
#### Import data
df <- read_csv("../data/quick_decisions_1_clean_data.csv")
colfactor <- c("condition", "ID", "FastorSlow", "measure")
df[colfactor] <- lapply(df[colfactor], as.factor)

## some participants did not respond to every question
## Making columns for those with NAs as placeholders
index_mapping <- df %>%
  select(trial_index, FastorSlow, measure) %>%
  distinct() %>%
  drop_na()

 # Create a complete data frame ensuring all trial indexes (4-25) exist for each ID
complete_data <- df %>%
  complete(ID, trial_index = 4:25, fill = list(response = NA)) %>%
  left_join(index_mapping, by = "trial_index") %>%
  group_by(ID) %>%
  mutate(
    condition = first(condition, order_by = trial_index)
  ) %>%
  ungroup()

 # Overwrite the original `FastorSlow` and `measure` columns with the mapped values
complete_data <- complete_data %>%
  mutate(
    FastorSlow = FastorSlow.y,
    measure = measure.y
  ) %>%
  select(-FastorSlow.y, -measure.y, -FastorSlow.x, -measure.x)  
  # Remove the temporary columns
```

```{r include = F}
# filter out rows that are Bria's or the TA's responses

complete_data <- complete_data %>%
  group_by(ID) %>%
  filter(!any(measure == "feedback" & grepl("Bria|Mihir", response_reverse))) %>%
  ungroup()

filtered_data <- complete_data %>%
  group_by(ID) %>%
  filter(!any(is.na(response))) %>%
  ungroup()
```

Our first step in our data cleaning was to obtain the subset of data from participants who had at least one non-NA response in each unique combination of Quickness, Character Judgments, and Decision Speed conditions.

```{r}
unique_ids <- complete_data %>%
  # Focus only on the relevant combinations of measure and FastorSlow
  filter(measure %in% c("quickness", "character") & FastorSlow %in% c("f", "s")) %>%
  group_by(ID) %>%
  # Check if each ID has at least one non-NA response for all combinations
  filter(all(!is.na(response[measure == "quickness" & FastorSlow == "f"])) &
         all(!is.na(response[measure == "quickness" & FastorSlow == "s"])) &
         all(!is.na(response[measure == "character" & FastorSlow == "f"])) &
         all(!is.na(response[measure == "character" & FastorSlow == "s"]))) %>%
  ungroup() %>%
  distinct(ID)

# Filter the original dataset to keep only rows with those IDs
filtered_data <- complete_data %>%
  filter(ID %in% unique_ids$ID)

distinct_id_count <- filtered_data %>% summarise(count = n_distinct(ID))
uni_num <- distinct_id_count[[1]]
```

For our groups' data, there are `r uni_num` unique participants (out of the original 100) that have at least one non-NA response in each unique combination of Quickness, Character Judgments, and Decision Speed conditions. Next, we excluded data from any participants who (a) failed our attention check, (b) incorrectly rated the slow condition as faster or equal to the quick condition, and/or (c) responded the same way for all non-NA responses.

```{r}
#### Data exclusion / filtering

exclu_data <- filtered_data %>%
  group_by(ID) %>%  # Group by ID
  
# Response should contain "nate" or "justin" in rows where measure = "attention"
  filter(any(measure == "attention" & grepl("nate|justin", response, ignore.case = TRUE))) %>%
  
# removed any ids that rate the fast person as slower than the slow person
  filter(!any(measure == "quickness" &
              FastorSlow == "f" &
              response_reverse < response_reverse[measure == "quickness" & FastorSlow == "s"])) %>%
  
# filter out ids that have all the same non-na response values
  filter(n_distinct(response, na.rm = TRUE) > 1) %>%
  
  ungroup() # Ungroup after filtering


distinct_id_count <- exclu_data %>% summarise(count = n_distinct(ID))
finss <- distinct_id_count[[1]]
```

For our groups' data, filtering out responses based on the previously specified exclusion criteria left us with a final sample size of `r finss` particpants' responses.


```{r}
#### Prepare data for analysis - create columns etc.

filtered_data <- exclu_data %>%
  filter(measure %in% c("quickness", "character"))


# Calculate averages for "character" measure grouped by ID and FastorSlow
character_averages <- filtered_data %>%
  filter(measure == "character") %>%
  group_by(ID, FastorSlow, condition) %>%
  summarize(
    response_reverse = mean(as.numeric(response_reverse), na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(measure = "character") # Ensure the measure column remains "character"


# Keep only the "quickness" rows
quickness_data <- filtered_data %>%
  filter(measure == "quickness") %>%
  select(ID, FastorSlow, condition, response_reverse, measure)


# Combine the averaged "character" data with the "quickness" data
tidydf <- rbind(quickness_data, character_averages)
```

```{r}
## Descriptive Statistics

# Determine whether we had a balanced design

## sample size of each randomized condition group
tidydf %>%
  group_by(condition) %>%
  summarize(subject_count = n_distinct(ID), .groups = "drop")
```
Even though our group's final sample size was quite small due to all of the exclusion criteria, it still ended up being a fairly balanced design.


#### Confirmatory analysis

```{r, include = F, echo = FALSE}
# Creating the Bar Plot to Assess Evidence of an Interaction

# Filter data to include only rows where measure is "character"
character_data <- tidydf %>%
  filter(measure == "character") %>%
  mutate(response_reverse = as.numeric(response_reverse), # Ensure response is numeric
         FastorSlow = as.factor(FastorSlow), # Convert FastorSlow to factor
         condition = as.factor(condition), # Convert condition to factor
         ID = as.factor(ID)) # Convert ID to factor


# Create the bar plot
ggplot(character_data, 
       aes(x = condition, y = response_reverse, fill = FastorSlow)) +
  stat_summary(fun = mean, # Compute mean of response
               geom = "bar", # Use bars to represent the summary
               position = position_dodge(width = 0.8), width = 0.8) +
  stat_summary(fun.data = mean_se, # Add error bars (mean ± standard error)
               geom = "errorbar",
               position = position_dodge(width = 0.8), width = 0.2) +
  labs(title = "Character Judgements by   \n Morality Condition and Decision Speed",
       x = "Morality Condition",
       y = "(Positive) Moral Character Evaluation",
       fill = "Decision Speed") +
  scale_fill_manual(values = c("f" = "orange1", "s" = "magenta3"), #Custom colors
                    labels = c("f" = "Quick", "s" = "Slow")) +
  scale_y_continuous(breaks = 1:7) +
  coord_cartesian(ylim = c(1, 7)) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.position = "right",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10))
```

```{r}
### Data Visualization (Our Group Only)

# Filter data to include only rows where measure is "character"
character_data <- tidydf %>%
  filter(measure == "character") %>%
  mutate(response_reverse = as.numeric(response_reverse), # Ensure response is numeric
         FastorSlow = as.factor(FastorSlow), # Convert FastorSlow to factor
         condition = as.factor(condition), # Convert condition to factor
         ID = as.factor(ID)) # Convert ID to factor


# Create the bar plot
barplot <- ggplot(character_data, 
       aes(x = condition, y = response_reverse, fill = FastorSlow)) +
  stat_summary(fun = mean, # Compute mean of response
               geom = "bar", # Use bars to represent the summary
               position = position_dodge(width = 0.8), width = 0.8) +
  stat_summary(fun.data = mean_se, # Add error bars (mean ± standard error)
               geom = "errorbar",
               position = position_dodge(width = 0.8), width = 0.2) +
  labs(x = "Morality Condition",
       y = "(Positive) Moral Character Evaluation",
       fill = "Decision Speed") +
  scale_fill_manual(values = c("f" = "orange1", "s" = "magenta3"),
                    labels = c("f" = "Quick", "s" = "Slow")) +
  scale_y_continuous(breaks = 1:7) +
  coord_cartesian(ylim = c(1, 7)) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.position = "none")


# Create a line plot
lineplot <- ggplot(character_data, 
       aes(x = condition, y = response_reverse, 
           color = FastorSlow, group = FastorSlow)) +
  stat_summary(fun = mean, # Compute mean of response
               geom = "line", # Use lines to connect the means
               linewidth = 1) +
  stat_summary(fun = mean, # Add points for the means
               geom = "point",  size = 3) +
  stat_summary(fun.data = mean_se, # Add error bars (mean ± standard error)
               geom = "errorbar", width = 0.4) +
  labs(x = "Morality Condition",
       y = "(Positive) Moral Character Evaluation",
       color = "Decision Speed") +
  scale_color_manual(values = c("f" = "orange1", "s" = "magenta3"), 
                     labels = c("f" = "Quick", "s" = "Slow")) +
  scale_y_continuous(breaks = 1:7) +
  coord_cartesian(ylim = c(1, 7)) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.position = "right",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10))


# Combine plots using patchwork
combined_plot <- (barplot + lineplot) +
  plot_layout(widths = c(2, 1)) + # Bar plot takes up 2/3, line plot 1/3
  plot_annotation(
    title = "Character Judgements by Morality Condition and Decision Speed  \n (Group Data Only)",
    theme = theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold")))


# Display the combined plot
print(combined_plot)
```

```{r}
## Running the ANOVA Interaction Test

# Fit a mixed model (accounts for ID) with the interaction of interest
intmod <- lmer(response_reverse ~ condition * FastorSlow + (1 | ID), 
               data = character_data)

# Use a Type III, 2x2 Two-way ANOVA test
# the results will tell us if the interaction term in needed in the model
Anova(intmod, type = "III")
```

Despite having such a small sample size with just our groups data, the study still replicated!


#### Model Diagnostics

**Independence Assumption**
Assessing *Independence of Observations* is a thought exercise. No participants responses should impact another participants responses, therefore there is no reason to suspect violations of this assumption.


To assess *Linearity* and *Homogeneity of Variance*, the Residuals vs Fitted plot will be investigated.

```{r}
fitted_vals <- fitted(intmod)
residuals <- resid(intmod)

plot(fitted_vals, residuals,
    pch = 19, col = "black",
    main = "Residuals vs Fitted",
    xlab = "Fitted Values",
    ylab = "Residuals")
abline(h = 0, col = "black")  # Horizontal line at 0
lines(lowess(fitted_vals, residuals), col = "red", lwd = 2)
```

**Linearity Assumption**
The red line does a great job of following the horizontal zero line, indicating little evidence of any missed curvature that our model would fail to account for. Therefore, there is no evidence to suspect any violations of this assumption.

**Homoskedacity**
To assess homogeneity of variance, we are looking for drastic changes in vertical spread of point on the residual vs. fitted plot. AS fitted values increase, we see a slight decreasing fanning pattern. This suggests slight evidence against our equal variance assumption, but not extreme enough to be problematic for our results.


**Normality of Residuals**
To assess *Normality of Residuals*, the Normal Q-Q Plot and a histogram of residuals will be referenced.
```{r}
# Normal Q-Q Plot
qqplot <- ggplot(data.frame(resid = resid(intmod)), aes(sample = resid)) +
  stat_qq(size = 1.5, shape = 1) +  # Adjust point size and shape
  stat_qq_line() +  # Add the Q-Q line
  labs(title = "Normal Q-Q Plot",
       x = "Theoretical Quantiles",
       y = "Sample Quantities") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title = element_text(size = 11), 
        axis.text = element_text(size = 11), 
        aspect.ratio = 1 / 2)  # Makes the plot rectangular (width > height)


# Histogram of Residuals
eij = residuals(intmod)
normhist <- ggplot(data.frame(resid = eij), aes(x = resid)) +
  geom_histogram(binwidth = 0.5, color = "black", fill = "lightgray") +
  labs(title = "Histogram of Residuals", 
       x = "Residuals", 
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title = element_text(size = 11),
        axis.text = element_text(size = 11),
        aspect.ratio = 2 / 3)  # Makes the plot rectangular (width > height)


# Combine plots using patchwork
normresid_plot <- qqplot + normhist +
  plot_layout(ncol = 2)


# Display the combined plot
print(normresid_plot)
```
The Normal Q-Q plot and Histogram of Residuals both show evidence of a light-tailed (short-tailed) pattern. This means that low values are higher than we expect, and high values are lower than we would expect. This is actually really good, meaning our residuals are even less problematic than expected! Therefore, there are no violations of the normality assumption.


**Multicollinearity**
The potential for *Multicollinearity* was explored by investigating the variance inflation factors (vifs).

```{r}
car::vif(intmod)
```
```{r}
sqrt(2.882353)
```
The square root of the interaction's variance inflation factor tells us that the standard error for the interaction term is 1.7 times larger due to multicollinearity (the sharing of information) with other variables in the model than it would have been otherwise. 
